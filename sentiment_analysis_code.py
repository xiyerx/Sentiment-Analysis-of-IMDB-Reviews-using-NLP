# -*- coding: utf-8 -*-
"""Sentiment_Analysis_code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XuObbbA3N8j_yzc2DGAfBqrM-AdPYjvk
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import nltk

data=pd.read_csv("IMDB Dataset.csv")
data.head(10)

#checking for NAs
data.isnull().sum()

#converting to lowercase
data['review'] = data['review'].str.lower()
data.head()

data['sentiment'].unique()

#converting 'positive' to 1, 'negative' to 0
data['sentiment'].replace({'positive':1, 'negative':0}, inplace=True)
data.head()

#removing html markups, eg <br>...</br>
import re
def remove_bracket_text(text):
    return re.sub('\<.*?\>', '', text)

data['review'] = data['review'].apply(remove_bracket_text)
data.head()

#removing special characters
def remove_sp_char(text):
    return re.sub(r'[^a-zA-z0-9\s]', '', text)

data['review']=data['review'].apply(remove_sp_char)
data.head(10)

pos = data[data['sentiment'] == 1]
neg = data[data['sentiment'] == 0]

#generating word cloud for positive reviews
from wordcloud import WordCloud
text = " ".join(pos['review'])
wc = WordCloud().generate(text)
plt.imshow(wc)
plt.axis('off')

#generating word cloud for negative reviews
text = " ".join(neg['review'])
wc = WordCloud().generate(text)
plt.imshow(wc)
plt.axis('off')

#TEXT PREPROCESSING

#tokenization
data['review'] = data['review'].str.split()
data.head(15)

#removing redundant words like 'movie' and 'film'
def rem(text):
  no_use_words = ['movies','films','movie','film']
  return [word for word in text if word not in no_use_words]

data['review'] = data['review'].apply(rem)
data.head(15)

nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words = stopwords.words('english')
stop_words[1:10]

#removing stopwords
def stop(text):
  return [word for word in text if word not in stop_words]

data['review'] = data['review'].apply(stop)
data.head(10)

#lemmatizing
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

def lemmit_it(text):
  return [lemmatizer.lemmatize(word, pos='v') for word in text]

data['review'] = data['review'].apply(lambda x: ' '.join(lemmit_it(x)))
data.head(15)

#stemming
#from nltk.stem import PorterStemmer
#ps = PorterStemmer()

#def stemmer(text):
  #return [ps.stem(word) for word in text]
#data['review'] = data['review'].apply(stemmer)
#data.head(15)

pos = data[data['sentiment'] == 1]
neg = data[data['sentiment'] == 0]

#generating word cloud for positive reviews after text pre-processing
from wordcloud import WordCloud
text = " ".join(pos['review'])
wc = WordCloud().generate(text)
plt.imshow(wc)
plt.axis('off')

#generating word cloud for negative reviews after text pre-processing
text = " ".join(neg['review'])
wc = WordCloud().generate(text)
plt.imshow(wc)
plt.axis('off')

#count of positive reviews vs negative reviews
x=[pos['sentiment'].count(), neg['sentiment'].count()]
pie = plt.pie(x, labels = ["Positive", "Negative"], autopct ='%1.1f%%')

#creating test and train datasets
print(data.shape)
nrows,ncols = data.shape
train_data = data.head(int(nrows*0.75))
test_data = data.tail(int(nrows*0.25))

from sklearn.metrics import accuracy_score, confusion_matrix

#to print accuracy and plot confusion matrix
def evaluate(y_true, y_pred):
  accuracy = round(accuracy_score(y_true, y_pred)*100, 2)
  cm = confusion_matrix(y_true, y_pred)
  print("Accuracy: ",accuracy,"%")
  print("Confusion Matrix:\n",cm)
  sns.heatmap(cm, annot=True, fmt='g')

from nltk.sentiment import SentimentIntensityAnalyzer
from tqdm.notebook import tqdm
nltk.download('vader_lexicon')

#SIA uses bag-of-words approach
sia = SentimentIntensityAnalyzer()
print(sia.polarity_scores('I am so happy!'))
print(sia.polarity_scores('This is the worst thing ever.'))

res=[]
for text in data['review']:
  res.append(sia.polarity_scores(text))

rating=[]
for x in res:
  rating.append(1) if x['compound'] >= 0 else rating.append(0)

data["score"] = rating
data.head(20)

evaluate(data['sentiment'], data['score'])

#FEATURE EXTRACTION

from gensim.test.utils import common_texts
from gensim.models import Word2Vec

#text vectorization using Word2Vec
x=data['review'].str.split()
w2v = Word2Vec(x, min_count=1,vector_size = 15, sg=0)
print(w2v.wv['good'])   #vector representation of any word

print(w2v.wv.similarity('good', 'like'))   #similarity between two words
print(w2v.wv.most_similar('good', topn=10))   #get other similar words
plt.figure(figsize=(13,5))
plt.xlabel("Words Similar to 'good'")
plt.ylabel("Similarity Score")
plt.scatter(*zip(*w2v.wv.most_similar('good', topn=10)))

#text vectorization using Count Vectorizer (bag-of-words)
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
vectorizer = CountVectorizer()

cv_train = vectorizer.fit_transform(train_data['review'])
print(vectorizer.get_feature_names_out())
print('CV_train:',cv_train.shape)

df = pd.DataFrame(data=cv_train.toarray(),columns = vectorizer.get_feature_names_out())
print(df)

cv_test = vectorizer.transform(test_data['review'])
print('CV_test:',cv_test.shape)
df = pd.DataFrame(data=cv_test.toarray(),columns = vectorizer.get_feature_names_out())
print(df[["like","love","good","best","bad","dumb","hate","disappoint"]])

#text vectorization using TF-IDF

tv=TfidfVectorizer(min_df=0,max_df=1, use_idf=True, ngram_range=(1,3))
tv_train=tv.fit_transform(train_data['review'])
tv_test=tv.transform(test_data['review'])

#modelling CV using Logistic Regression
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(cv_train,train_data['sentiment'])
pred = lr.predict(cv_test)
print(pred[1:20])

evaluate(test_data['sentiment'], pred)

#modelling TF-IDF using Logistic Regression
lr.fit(tv_train,train_data['sentiment'])
pred = lr.predict(tv_test)

evaluate(test_data['sentiment'], pred)

#modelling CV using Bernoulli's Naive Bayes
from sklearn.naive_bayes import BernoulliNB
bnb = BernoulliNB(alpha = 2)
bnb.fit(cv_train,train_data['sentiment'])
pred = bnb.predict(cv_test)

evaluate(test_data['sentiment'], pred)

#modelling TF-IDF using Bernoulli's Naive Bayes
bnb.fit(tv_train,train_data['sentiment'])
pred = bnb.predict(tv_test)

evaluate(test_data['sentiment'], pred)

#modelling CV using Multinomial Naive Bayes
from sklearn.naive_bayes import MultinomialNB
mnb = MultinomialNB()
mnb.fit(cv_train,train_data['sentiment'])
pred = mnb.predict(cv_test)

evaluate(test_data['sentiment'], pred)

#modelling TF-IDF using Multinomial Naive Bayes
from sklearn.naive_bayes import MultinomialNB
mnb = MultinomialNB()
mnb.fit(tv_train,train_data['sentiment'])
pred = mnb.predict(tv_test)

evaluate(test_data['sentiment'], pred)

#modelling CV using Linear SVM
from sklearn.svm import SVC
svc = SVC(kernel='linear') 
svc.fit(cv_train,train_data['sentiment'])
pred = svc.predict(cv_test)

evaluate(test_data['sentiment'], pred)

#modelling TF-IDF using Linear SVM
svc.fit(tv_train,train_data['sentiment'])
pred = svc.predict(tv_test)

evaluate(test_data['sentiment'], pred)